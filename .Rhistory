plot_1 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line")
plot_1
plot_2 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
xend = Gr_Liv_Area, yend = .fitted),
alpha = 0.3) +
geom_point(size = 1, alpha = 0.3) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line (with residuals)")
plot_2
grid.arrange(plot_1, plot_2, nrow = 1)
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra)
# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
library(h2o)       # for resampling and model training
# Model interpretability packages
library(vip)      # variable importance
library(AmesHousing)
# Access the Ames dataset
ames_data <- make_ames()
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames_data, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
# start modeling of the data
# model 1 - simple
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
summary(model1)
sigma(model1)
confint(model1, level = 0.95)
# model 2
(model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train))
# or
(model2 <- update(model1, . ~ . + Year_Built))
summary(model2)
#interaction
lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)
# model 3
# include all possible main effects
model3 <- lm(Sale_Price ~ ., data = ames_train)
broom::tidy(model3)
# drawing simple graphs
plot_1 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line")
plot_1
plot_2 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
xend = Gr_Liv_Area, yend = .fitted),
alpha = 0.3) +
geom_point(size = 1, alpha = 0.3) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line (with residuals)")
plot_2
grid.arrange(plot_1, plot_2, nrow = 1)
shiny::runApp('~/WORK/CEMA/MAP-AMR/Dashboard/appdir')
gc()
gc()
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra)
# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
library(h2o)       # for resampling and model training
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra)
# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
library(h2o)       # for resampling and model training
# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
library(h2o)       # for resampling and model training
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra)
# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
#library(h2o)       # for resampling and model training
# Model interpretability packages
library(vip)      # variable importance
library(AmesHousing)
# Access the Ames dataset
ames_data <- make_ames()
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames_data, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
# model 1 - simple
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
summary(model1)
sigma(model1)
confint(model1, level = 0.95)
# model 2
(model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train))
# or
(model2 <- update(model1, . ~ . + Year_Built))
summary(model2)
#interaction
lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)
# model 3
# include all possible main effects
model3 <- lm(Sale_Price ~ ., data = ames_train)
broom::tidy(model3)
# drawing simple graphs
plot_1 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line")
plot_1
plot_1
# drawing simple graphs
plot_1 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line")
plot_1
# model 1 - simple
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
# drawing simple graphs
plot_1 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line")
gc()
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra)
# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
#library(h2o)       # for resampling and model training
# Model interpretability packages
library(vip)      # variable importance
library(AmesHousing)
# Access the Ames dataset
ames_data <- make_ames()
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames_data, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
# drawing simple graphs
plot_1 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line")
plot_1
plot_2 <- model1 %>%
broom::augment() %>%
ggplot(aes(Gr_Liv_Area, Sale_Price)) +
geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
xend = Gr_Liv_Area, yend = .fitted),
alpha = 0.3) +
geom_point(size = 1, alpha = 0.3) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "lm") +
scale_y_continuous(labels = scales::dollar) +
ggtitle("Fitted regression line (with residuals)")
plot_2
grid.arrange(plot_1, plot_2, nrow = 1)
set.seed(123)
(cv_model1 <- train(
form = Sale_Price ~ Gr_Liv_Area,
data = ames_train,
method = "lm",
trControl = tainControl(method = "cv", number = 10)
))
set.seed(123)
(cv_model1 <- train(
form = Sale_Price ~ Gr_Liv_Area,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 10)
))
# cv for model 2
set.seed(123)
(cv_model2 <- train(
form = Sale_Price ~ Gr_Liv_Area + Year_Built,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 10)
))
# cv for model 3
set.seed(123)
(cv_model3 <- train(
form = Sale_Price ~ Gr_Liv_Area * Year_Built,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 10)
))
# cv for model 3
set.seed(123)
(cv_model3 <- train(
form = Sale_Price ~ .,
data = ames_train,
method = "lm",
trControl = trainControl(method = "cv", number = 10)
))
# Extract out of sample performance measures
summary(resamples(list(
model1 = cv_model1,
model2 = cv_model2,
model3 = cv_model3
)))
set.seed(123)
cv_model_pcr <- train(
Sale_Price ~ .,
data = ames_train,
method = "pcr",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 100
)
cv_model_pcr <- train(
Sale_Price ~ .,
data = ames_train,
method = "pcr",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 100
)
set.seed(123)
cv_model_pcr$bestTune
cv_model_pcr$results %>%
dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))
# plot cross-validated RMSE
ggplot(cv_model_pcr)
# perform 10-fold cross validation on a PLS model tuning the
# number of principal components to use as predictors from 1-30
set.seed(123)
cv_model_pls <- train(
Sale_Price ~ .,
data = ames_train,
method = "pls",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 30
)
# model with lowest RMSE
cv_model_pls$bestTune
##    ncomp
## 20    20
# results for model with lowest RMSE
cv_model_pls$results %>%
dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
##   ncomp     RMSE  Rsquared      MAE   RMSESD RsquaredSD   MAESD
## 1    20 25459.51 0.8998194 16022.68 5243.478 0.04278512 1665.61
# plot cross-validated RMSE
ggplot(cv_model_pls)
vip(cv_model_pls, num_features = 20, method = "model")
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting
library(rsample)   # for data splitting
# Modeling packages
library(caret)     # for logistic regression modeling
# Model interpretability packages
library(vip)       # variable importance
# data
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
exists("attrition")
library(Attrition)
install.packages("Attrition")
model1 <- glm(Attrition ~ MonthlyIncome, family = "binomial", data = churn_train)
library(tidymodels)
install.packages("tidymodels")
library(tidymodels)
# Check available datasets
data(package = "modeldata")
# Load attrition dataset
data("attrition", package = "modeldata")
head(attrition)
# data
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
model1 <- glm(Attrition ~ MonthlyIncome, family = "binomial", data = churn_train)
model2 <- glm(Attrition ~ OverTime, family = "binomial", data = churn_train)
summary(model1)
tidy(model1)
tidy(model2)
# it is easier to interprete the coefficents using the exp() transformation
exp(coef(model1))
exp(coef(model2))
# multiple logistic regression
model3 <- glm(
Attrition ~ MonthlyIncome + OverTime,
family = "binomial",
data = churn_train
)
tidy(model3)
# plot for model 3
plot_model_3 <- model3 %>%
ggplot(aes(Attrition, MonthlyIncome)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "glm")
plot_model_3
# plot for model 3
plot_model_3 <- model3 %>%
broom::augment() %>%
ggplot(aes(Attrition, MonthlyIncome)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "glm")
plot_model_3
# model accuracy
set.seed(123)
cv_model1 <- train(
Attrition ~ MontlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainingControl(method = "cv", number = 10)
)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainingControl(method = "cv", number = 10)
)
# Modeling packages
library(caret)     # for logistic regression modeling
# Model interpretability packages
library(vip)       # variable importance
# Access the Ames dataset
ames_data <- make_ames()
library(tidymodels)
# Check available datasets
data(package = "modeldata")
# Load attrition dataset
data("attrition", package = "modeldata")
head(attrition)
# data
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
# Create training (70%) and test (30%) sets for the
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
model1 <- glm(Attrition ~ MonthlyIncome, family = "binomial", data = churn_train) # using the glm function
model2 <- glm(Attrition ~ OverTime, family = "binomial", data = churn_train)
tidy(model1)
tidy(model2)
# it is easier to interprete the coefficents using the exp() transformation
exp(coef(model1))
exp(coef(model2))
# Thus, the odds of an employee attriting in model1 increase multiplicatively by 0.9999 for every one dollar increase in MonthlyIncome, whereas the odds of attriting in model2 increase multiplicatively by 4.0812 for employees that work OverTime compared to those that do not.
# multiple logistic regression
model3 <- glm(
Attrition ~ MonthlyIncome + OverTime,
family = "binomial",
data = churn_train
)
tidy(model3)
# plot for model 3
plot_model_3 <- model3 %>%
broom::augment() %>%
ggplot(aes(Attrition, MonthlyIncome)) +
geom_point(size = 1, alpha = 0.3) +
geom_smooth(se = FALSE, method = "glm")
plot_model_3
# model accuracy
set.seed(123)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainingControl(method = "cv", number = 10)
)
cv_model1 <- train(
Attrition ~ MonthlyIncome,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model2 <- train(
Attrition ~ MonthlyIncome + OverTime,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model3 <- train(
Attrition ~ .,
data = churn_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
# extract out of sample performance measures
summary(
resamples(
list(
model1 = cv_model1,
model2 = cv_model2,
model3 = cv_model3
)
)
)$statistics$Accuracy
# predict class
pred_class <- predict(cv_model3, churn_train)
# create confusion matrix
confusionMatrix(
data = relevel(pred_class, ref = "Yes"),
reference = relevel(churn_train$Attrition, ref = "Yes")
)
library(ROCR)
install.packages("ROCR")
library(ROCR)
m1_prop <- predict(cv_model1, churn_train, type = "prop")$Yes
m1_prop <- predict(cv_model1, churn_train, type = "prob")$Yes
# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
m1_prob <- predict(cv_model1, churn_train, type = "prob")$Yes
m3_prob <- predict(cv_model3, churn_train, type = "prob")$Yes
# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m3_prob, churn_train$Attrition) %>%
performance(measure = "tpr", x.measure = "fpr")
# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
legend(0.8, 0.2, legend = c("cv_model1", "cv_model3"),
col = c("black", "blue"), lty = 2:1, cex = 0.6)
# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
# Perform 10-fold CV on a PLS model tuning the number of PCs to
# use as predictors
set.seed(123)
cv_model_pls <- train(
Attrition ~ .,
data = churn_train,
method = "pls",
family = "binomial",
trControl = trainControl(method = "cv", number = 10),
preProcess = c("zv", "center", "scale"),
tuneLength = 16
)
# Model with lowest RMSE
cv_model_pls$bestTune
##    ncomp
## 14    14
# results for model with lowest loss
cv_model_pls$results %>%
dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
##   ncomp  Accuracy     Kappa AccuracySD   KappaSD
## 1    14 0.8757518 0.3766944 0.01919777 0.1142592
# Plot cross-validated RMSE
ggplot(cv_model_pls)
vip(cv_model3, num_features = 20)
