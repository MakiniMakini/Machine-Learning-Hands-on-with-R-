---
title: "linear_regression"
author: "Makini Makini"
date: "2024-09-08"
output: html_document
---

Libraries and data 
```{r}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(gridExtra)

# Modeling packages
library(caret)    # for cross-validation, etc.
library(rsample)   # for resampling procedures
#library(h2o)       # for resampling and model training
# Model interpretability packages
library(vip)      # variable importance

library(AmesHousing)

# Access the Ames dataset
ames_data <- make_ames()

# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames_data, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

```{r}
# model 1 - simple
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)

# summary(model1) 
# sigma(model1)
# confint(model1, level = 0.95)

# model 2
(model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train))
# or
# (model2 <- update(model1, . ~ . + Year_Built))
# summary(model2)

#interaction
lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)

# model 3
# include all possible main effects
model3 <- lm(Sale_Price ~ ., data = ames_train)
broom::tidy(model3)
```


```{r}
# drawing simple graphs
plot_1 <- model1 %>%
  broom::augment() %>% 
  ggplot(aes(Gr_Liv_Area, Sale_Price)) +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Fitted regression line")

plot_1

plot_2 <- model1 %>% 
  broom::augment() %>% 
  ggplot(aes(Gr_Liv_Area, Sale_Price)) +
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = .fitted),
               alpha = 0.3) +
  geom_point(size = 1, alpha = 0.3) +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Fitted regression line (with residuals)")
  

plot_2
grid.arrange(plot_1, plot_2, nrow = 1)
```

Cross validation models
```{r}
set.seed(123)

(cv_model1 <- train(
  form = Sale_Price ~ Gr_Liv_Area,
  data = ames_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

# cv for model 2
set.seed(123)

(cv_model2 <- train(
  form = Sale_Price ~ Gr_Liv_Area + Year_Built,
  data = ames_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

# cv for model 3
set.seed(123)

(cv_model3 <- train(
  form = Sale_Price ~ .,
  data = ames_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1,
  model2 = cv_model2,
  model3 = cv_model3
)))
```

PCR
```{r}
set.seed(123)
cv_model_pcr <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 100
)
# model with lowest RMSE
cv_model_pcr$bestTune
# results for model with lowest RMSE
cv_model_pcr$results %>%
  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))

# plot cross-validated RMSE
ggplot(cv_model_pcr)
```


partial least squares
```{r}
# perform 10-fold cross validation on a PLS model tuning the 
# number of principal components to use as predictors from 1-30
set.seed(123)
cv_model_pls <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 30
)

# model with lowest RMSE
cv_model_pls$bestTune
##    ncomp
## 20    20

# results for model with lowest RMSE
cv_model_pls$results %>%
  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
##   ncomp     RMSE  Rsquared      MAE   RMSESD RsquaredSD   MAESD
## 1    20 25459.51 0.8998194 16022.68 5243.478 0.04278512 1665.61

# plot cross-validated RMSE
ggplot(cv_model_pls)
```


```{r}
vip(cv_model_pls, num_features = 20, method = "model")
```

